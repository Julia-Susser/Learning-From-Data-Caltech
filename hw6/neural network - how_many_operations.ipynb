{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import e\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calculate_xjLs:\n",
    "    def calculate_xjLs(self,x):\n",
    "        xjLs = [x]\n",
    "        sjLs = []\n",
    "        for layer in range(self.N_layers-1):\n",
    "            x_l = [1] if layer != self.N_layers-2 else [] #self.N_layers-2 bc layer 2nd gives layer 3 and layer 2nd is actually 1\n",
    "            s_l = []\n",
    "            weights_for_layer = self.weights[layer]\n",
    "            for w_jl in weights_for_layer: #picking specific weights of one output (j)\n",
    "                s_jl = np.matmul(x,w_jl.T)\n",
    "                self.count += (len(x))\n",
    "                x_jl = self.activation_func(np.matmul(x,w_jl.T))\n",
    "                x_l.append(x_jl)\n",
    "                s_l.append(s_jl)\n",
    "            xjLs.append(x_l)\n",
    "            sjLs.append(s_l)\n",
    "            x = x_l\n",
    "        return xjLs, sjLs\n",
    "\n",
    "class CalculateDeltas:\n",
    "    def calculate_deltaL(self,x,y):\n",
    "        x_1_L = self.xjLs[self.N_layers-1][0]\n",
    "        return [(x_1_L-y)*self.derivative_of_activation_func(x_1_L)]\n",
    "    \n",
    "    def calculate_deltas(self,layer,deltas_layer):\n",
    "        #when layer is three gives deltas for two (already made deltaL), when layer is 2 it gives deltas for 1 --> stop at layer 2\n",
    "        if layer == 2:\n",
    "            return [deltas_layer]\n",
    "        layer_m1 = layer-1\n",
    "        d_layer = [1] if layer == self.N_layers else range(1,self.layers[layer_m1]) \n",
    "        d_layer_m1 = range(1,self.layers[layer_m1-1]) \n",
    "        \n",
    "        deltas_layer_m1_i = []\n",
    "        \n",
    "        for i in d_layer_m1:\n",
    "            sum_of_j_paths = 0\n",
    "            for j in d_layer:\n",
    "                deltaj = deltas_layer[j-1]\n",
    "                sum_of_j_paths += deltaj*self.weights[layer_m1-1][j-1][i] \n",
    "                self.count += 1\n",
    "                #j-1 because the weights for j=1 are actually the first weights as j=0 is 1\n",
    "            \n",
    "            delta_layer_m1_i = self.derivative_of_activation_func(self.sjLs[layer_m1-2][i-1]) * sum_of_j_paths #-2 becuase no sjs for layer 1\n",
    "            deltas_layer_m1_i.append(delta_layer_m1_i)\n",
    "            \n",
    "        return self.calculate_deltas(layer-1,deltas_layer_m1_i) + [deltas_layer]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot:\n",
    "    def simple_plot(self,X,preds):\n",
    "        data = np.concatenate([np.array(X),np.array(preds).reshape(-1,1)],axis=1)\n",
    "        df = pd.DataFrame(data, columns = [\"x1\",\"x2\",\"y\"])\n",
    "        sns.scatterplot(data=df, x=\"x1\", y=\"x2\", hue=\"y\")\n",
    "        plt.xlim([-1, 1])\n",
    "        plt.ylim([-1, 1])\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_ein(self):\n",
    "        preds = self.calculate_preds(self.Xorig)\n",
    "        if self.activation == \"tahn\": preds = np.sign(preds)\n",
    "        self.simple_plot(self.Xorig, preds)\n",
    "     \n",
    "    def plot_eout(self):\n",
    "        X = np.random.uniform(-1.0,1.0,(1000,2))\n",
    "        preds = self.calculate_preds(X)\n",
    "        if self.activation == \"tahn\": preds = np.sign(preds)\n",
    "        y = self.find_actual_y(X)\n",
    "        self.simple_plot(X, preds)\n",
    "        self.simple_plot(X, y)\n",
    "        preds = self.calculate_preds(X)\n",
    "        plt.plot(preds,preds,\"go\")\n",
    "        plt.plot(preds,y,\"bo\")\n",
    "        plt.vlines(preds,preds,y, colors='k', linestyles='solid', label='')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Line():\n",
    "    def create_line(self):\n",
    "        if self.type == \"linear regressor\":\n",
    "            self.mbs = np.random.uniform(10.0,0.0,self.input+1)\n",
    "        if self.type == \"PLA\":\n",
    "            self.mbs = np.random.uniform(1.0,0.0,self.input-1+1)\n",
    "        if self.type == \"circle\":\n",
    "            self.r = np.random.uniform(.8,0.0,1)\n",
    "    def find_actual_y(self,X):\n",
    "        if self.type == \"PLA\":\n",
    "            return np.sign(np.matmul(self.add_thresholdCol(X)[:,:-1],self.mbs.T) - X[:,-1])\n",
    "        if self.type == \"linear regressor\":\n",
    "            return np.matmul(self.add_thresholdCol(X),self.mbs.T)\n",
    "        if self.type == \"circle\":\n",
    "            return [1 if x else -1 for x in np.sum(X**2,axis=1)>self.r**2]\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDGradient:\n",
    "    \n",
    "    def sgd_gradient(self):\n",
    "        \n",
    "        for layer in range(2,self.N_layers+1):\n",
    "            layer_m1 = layer-1 #wij_2 where 2=layer is the weights between layer 1 (layer-1) and layer 2 (layer)\n",
    "            delta_layer = self.deltas[layer-2] #delta layer 2 is the first delta at index 0\n",
    "            dj_l = self.layers[layer-1]\n",
    "            if layer==self.N_layers: dj_l +=1\n",
    "            for j in range(1,dj_l):\n",
    "                deltaj_l = delta_layer[j-1]\n",
    "                for i in range(self.layers[layer_m1-1]):\n",
    "                    wij_l = self.weights[layer_m1-1][j-1][i]#weights for layer 1 to go to layer 2 is at index 0\n",
    "                    xi_layer_m1 = self.xjLs[layer_m1-1][i]\n",
    "                    gradient = -self.n*xi_layer_m1*deltaj_l\n",
    "                    self.count += 1\n",
    "                    self.weights[layer_m1-1][j-1][i] = wij_l + gradient\n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "    def sgd_epoch(self):\n",
    "        vals = list(range(self.N))\n",
    "        random.shuffle(vals)\n",
    "        for v in vals[:1]:\n",
    "            self.count = 0\n",
    "            x, y = self.X[v],self.y[v]\n",
    "            self.xjLs,self.sjLs = self.calculate_xjLs(x)\n",
    "            self.deltaL = self.calculate_deltaL(x,y)\n",
    "            self.deltas = self.calculate_deltas(self.N_layers,self.deltaL)\n",
    "            self.sgd_gradient()\n",
    "            print(self.count)\n",
    "        return self.weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunc:\n",
    "    def activation_func(self,sj):\n",
    "        if self.activation == \"relu\":\n",
    "            return max(0,sj)\n",
    "        if self.activation == \"tahn\":\n",
    "            return (e**sj-e**-sj)/(e**sj+e**-sj)\n",
    "\n",
    "    \n",
    "    def derivative_of_activation_func(self,s):\n",
    "        if self.activation == \"relu\":\n",
    "            return 0 if s<0 else 1\n",
    "        if self.activation == \"tahn\":\n",
    "            return (1-(self.activation_func(s))**2)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class NeuralNetwork(Line,Plot,ActivationFunc,Calculate_xjLs,CalculateDeltas,SGDGradient):\n",
    "    def __init__(self):\n",
    "        self.N = 100\n",
    "        self.input = 5\n",
    "        self.layers = [self.input+1,4,1]\n",
    "        self.n = .1\n",
    "        self.N_layers = len(self.layers)\n",
    "        self.type = \"linear regressor\"\n",
    "        self.activation = \"relu\"\n",
    "        self.init_weights()\n",
    "        self.createData()\n",
    "        \n",
    "        self.count = 0\n",
    "        \n",
    "        self.ein = self.mse(self.X,self.y)\n",
    "        print(\"Pre Adjusting Ein %s\" % self.ein)\n",
    "        self.runSGD()\n",
    "        \n",
    "        print(\"Final Weights %s\" % self.weights)      \n",
    "        print(\"Num Iterations %s\" %self.num_iter)\n",
    "        self.ein = self.mse(self.X,self.y)\n",
    "        print(\"Ein %s\" % self.ein)\n",
    "        \n",
    "        self.eout = self.eout()\n",
    "        print(\"Eout %s\" % self.eout)\n",
    "        \n",
    "        #self.plot_ein()\n",
    "        #self.plot_eout()\n",
    "        \n",
    "    \n",
    "      \n",
    "        \n",
    "####################################################\n",
    "    def runSGD(self):\n",
    "        self.num_iter = 0\n",
    "        hump = 0\n",
    "        for x in range(1):\n",
    "            wt = self.weights_list()\n",
    "            self.num_iter += 1\n",
    "            self.sgd_epoch()\n",
    "            wt1 = self.weights_list()\n",
    "            if np.sqrt(sum((wt-wt1)**2)) <.01: #np.linalg.norm\n",
    "                hump += 1\n",
    "                if hump>1000:\n",
    "                    break\n",
    "    def add_thresholdCol(self,X):\n",
    "        return np.concatenate([[[1]for x in range(len(X))],X],axis=1)\n",
    "    \n",
    "    def createData(self):\n",
    "        self.create_line()\n",
    "        self.Xorig = np.random.uniform(-1.0,1.0,(self.N,self.input))\n",
    "        self.y = self.find_actual_y(self.Xorig)\n",
    "        self.X = self.add_thresholdCol(self.Xorig)\n",
    "\n",
    "        \n",
    "        \n",
    "    def weights_list(self):\n",
    "        change = []\n",
    "        for layer in range(2,self.N_layers+1):\n",
    "                layer_m1 = layer -1\n",
    "                jss = [0] if layer == self.N_layers else range(self.layers[layer-1]-1)\n",
    "                iss = range(self.layers[layer_m1-1]) \n",
    "                change += [self.weights[layer_m1-1][j][i] for i in iss for j in jss]\n",
    "        return np.array(change) \n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.weights = []\n",
    "        for layer in range(1,self.N_layers):\n",
    "            N_l = self.layers[layer]-1 if layer+1 != self.N_layers else self.layers[layer]\n",
    "            N_lm1 = self.layers[layer-1]\n",
    "            self.weights.append(np.random.uniform(-1.0,1.0,(N_l,N_lm1)))\n",
    "        \n",
    "    def mse(self,X,y):\n",
    "        w = self.weights\n",
    "        return sum([(self.calculate_xjLs(X[v])[0][-1][0] - y[v])**2 for v in range(len(X))])/len(X)\n",
    "    \n",
    "    def calculate_preds(self,X):\n",
    "        X = self.add_thresholdCol(X)\n",
    "        return [self.calculate_xjLs(x)[0][-1][0] for x in X]\n",
    "        \n",
    "    \n",
    "    def eout(self):\n",
    "        Xorig = np.random.uniform(-1.0,1.0,(100,self.input))\n",
    "        X = self.add_thresholdCol(Xorig)\n",
    "        y = self.find_actual_y(Xorig)\n",
    "        return self.mse(X,y)\n",
    "\n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Adjusting Ein 134.11922597883415\n",
      "47\n",
      "Final Weights [array([[ 0.14811362, -1.06543579, -0.88463932, -1.09803737,  0.45332922,\n",
      "        -0.77225908],\n",
      "       [-1.40406797,  0.17980799, -0.35437695,  0.58452447, -1.56258608,\n",
      "         0.45126077],\n",
      "       [ 0.56830913,  0.48879526,  0.87498181,  0.40012446, -0.72059293,\n",
      "        -0.37618967]]), array([[ 0.13881588,  0.94110506, -0.62311481,  0.99119079]])]\n",
      "Num Iterations 1\n",
      "Ein 120.05073376769775\n",
      "Eout 94.72058367678396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NeuralNetwork at 0x7fe736f109a0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeuralNetwork()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
